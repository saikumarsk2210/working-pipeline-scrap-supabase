import { ApifyClient } from "apify-client";
import fs from "fs";
import { execSync } from "child_process";
import { createClient } from "@supabase/supabase-js";
import dotenv from "dotenv";

dotenv.config();
console.log("Starting pipeline script...");

// Initialize Apify client
const client = new ApifyClient({ token: process.env.APIFY_TOKEN });

// Initialize Supabase client
const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_KEY);

// 1Ô∏è‚É£ Run Apify Scraper
async function scrapeJobs() {
    console.log("‚úÖ Running Apify scraper...");
    const inputJSON = {
        country: "IN",
        followApplyRedirects: true,
        maxItems: 15,
        parseCompanyDetails: true,
        position: "Data Scientist, Business Analyst, Data Engineer, Python Developer, Full Stack Developer, Machine Learning Engineer, Software Engineer, Backend Developer, Frontend Developer, AI Engineer, DevOps Engineer",
        saveOnlyUniqueItems: true
    };

    try {
        const actorRun = await client.actor("hMvNSpz3JnHgl5jkh").call(inputJSON);

        if (!actorRun.defaultDatasetId) {
            console.error("‚ùå No dataset ID found.");
            process.exit(1);
        }

        const datasetId = actorRun.defaultDatasetId;
        const { items } = await client.dataset(datasetId).listItems();

        if (items.length === 0) {
            console.log("‚ö†Ô∏è No jobs scraped.");
        } else {
            fs.writeFileSync("scraped_jobs.json", JSON.stringify(items, null, 2));
            console.log("‚úÖ Scraped jobs saved.");
        }
    } catch (error) {
        console.error("‚ùå Scraping failed:", error);
        process.exit(1);
    }
}

// 2Ô∏è‚É£ Format Jobs (Runs Python Script)
function formatJobs() {
    console.log("‚úÖ Formatting job data...");
    try {
        execSync("python format_jobs.py", { stdio: "inherit" });
        console.log("‚úÖ Formatting complete.");
    } catch (error) {
        console.error("‚ùå Formatting failed:", error);
        process.exit(1);
    }
}

// 3Ô∏è‚É£ Upload Jobs to Supabase (With Duplicate Prevention)
async function uploadJobs() {
    console.log("‚úÖ Uploading jobs to Supabase...");

    try {
        const jobs = JSON.parse(fs.readFileSync("formatted_jobs.json", "utf8"));

        for (const job of jobs) {
            const { job_url, ...jobData } = job; // Separate job_url for checking

            // üîç Check if the job already exists in Supabase
            const { data: existingJob, error: fetchError } = await supabase
                .from("jobs")
                .select("job_url")
                .eq("job_url", job_url)
                .single();

            if (fetchError && fetchError.code !== "PGRST116") {
                console.error("‚ùå Error checking existing job:", fetchError);
                continue;
            }

            if (existingJob) {
                console.log(`‚ö†Ô∏è Skipping duplicate job: ${job.title}`);
                continue;
            }

            // üì• Insert new job
            const { error: insertError } = await supabase
                .from("jobs")
                .insert([{ job_url, ...jobData }]);

            if (insertError) {
                console.error("‚ùå Upload error:", insertError);
            } else {
                console.log(`‚úÖ Job '${job.title}' uploaded.`);
            }
        }
    } catch (error) {
        console.error("‚ùå Uploading failed:", error);
        process.exit(1);
    }
}

// 4Ô∏è‚É£ Run Full Pipeline
async function runPipeline() {
    await scrapeJobs();
    formatJobs();
    await uploadJobs();
    console.log("üöÄ Pipeline completed successfully!");
}

runPipeline();
